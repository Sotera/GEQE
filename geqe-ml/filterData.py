############# ############# ############# ############# ############## filterData# by JAG3############## ############# ############# ############# #############from pyspark import SparkConf, SparkContextfrom pyspark.sql import SQLContext, Rowfrom pyspark.sql.types import BooleanTypefrom datetime import dateimport sysimport argparsesys.path.insert(0, './lib/')from to_parquet import csvToDataFrameimport fspLibimport shapeReader# HARD CODE YOU INPUT DATA SETS AND DATA TYPESDATA_SETS = {"hdfs://xdata/qcr/gnip":66}LOWER_TIME = date(2006,03,21)UPPER_TIME = date(3000,01,01)if __name__ == "__main__":    parser = argparse.ArgumentParser()    parser.add_argument("shapeFile", help="The shape file path")    parser.add_argument("outputPath",help="Output destination")    parser.add_argument("-jobNm", help="Application name, default = 'Geqe Data Filter'",default='Geqe data filter.')    parser.add_argument("-cNum", type=int, help="Number of processes to coalesce initial input data to, default = 3",default = 50)    parser.add_argument("--stopWordsFile",help="File path to a stop words list. One word per line. default=inputFiles/stopWordList.txt",default="inputFiles/stopWordList.txt")    parser.add_argument("-sCustStop", help="Comma seperated list of stop words to add include on this run",default='')    args = parser.parse_args()    shapeFile = args.shapeFile    outputPath = args.outputPath    jobNm = args.jobNm    cNum = args.cNum    stopWordsPath = args.stopWordsFile    sCustStop = args.sCustStop    #Declare Spark Context    conf = SparkConf().setAppName(jobNm)    sc = SparkContext(conf = conf)    sqlContext = SQLContext(sc)    #Read in stop word list early to get notified of issues early in process    bc_lStopWords = fspLib.load_stopwords(sc,stopWordsPath,sCustStop)    #Create polygon list and broadcast variable based on it    lPolygon = shapeReader.readInShapeJson(shapeFile)    bc_lTargetPolygons = sc.broadcast(lPolygon)    # register SQL functions    sqlContext.registerFunction("hasScorableWord", lambda text: fspLib.hasScorableWord(text,True,bc_lStopWords),returnType=BooleanType())    sqlContext.registerFunction("inRegionOfInterest", lambda lat,lon: fspLib.inROI(lat,lon,bc_lTargetPolygons),returnType=BooleanType())    # read in each data set and filter for region / time of interest    dfs = []    i = 0    for dataSet,dataType in DATA_SETS.iteritems():        print 'processing data set: ',dataSet        i+=1        tablename = 'table'+str(i)        df = csvToDataFrame(sc,sqlContext,dataSet,dataType)        df.registerTempTable(tablename)        sqlCommand = "SELECT * from %s WHERE inRegionOfInterest(lat,lon) AND hasScorableWord(text) " % (tablename)        #sqlCommand = "SELECT * from %s WHERE hasScorableWord(text) " % (tablename)        df = sqlContext.sql(sqlCommand)        dfs.append(df)    # union ll dfs together    print 'performing union on all of: ', len(dfs)    unionDF = dfs[0]    if len(dfs) > 1:        for df in dfs[1:]:            print 'performing df union'            unionDF = unionDF.unionAll(df)    total_entries = unionDF.count()    print "JAG: TOTAL ENTRIES: ", total_entries        #Write data directly to ES    es_output = unionDF.map(lambda x: ('key',         {'source':'twitter',         'user':x.user,         'imageUrl':'',         'message':x.text,         'post_date':str(x.dt).replace(' ','T'),        'indexedDate':'2016-05-03',        'dataset':'ireland_uk',         'location':{            'type':'point',             'coordinates':(x.lon, x.lat)        }    }))    es_read_conf = {        "es.nodes"    : "10.1.92.76",        "es.port"     : "9200",        "es.resource" : "geqe_ire_uk_2/post"    }    es_output.saveAsNewAPIHadoopFile(        path='-',        outputFormatClass='org.elasticsearch.hadoop.mr.EsOutputFormat',        keyClass='org.apache.hadoop.io.NullWritable',        valueClass='org.elasticsearch.hadoop.mr.LinkedMapWritable',        conf = es_read_conf    )    ##The following block bins all tweets by lat lon, counts, and creates a csv file    #mapped = unionDF.map(lambda x: (str(int(x.lat)) + "_" + str(int(x.lon)), 1)).reduceByKey(lambda x,y: x+y).collect()    #f0 = open("geoCount.csv","w")    #for entry in mapped:    #    p = entry[0].find("_")    #    f0.write("\t".join([entry[0][:p], entry[0][p+1:], str(entry[1])])+"\n")    #unionDF.coalesce(cNum).write.parquet(outputPath)